{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b15b0881",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Microscopic Image Analysis\"\n",
    "teaching: 10\n",
    "exercises: 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f65ff",
   "metadata": {},
   "source": [
    "\n",
    "- How to work with the image data?\n",
    "- What are different ways to explore and visualise the image data?\n",
    "- How is image clustering performed?\n",
    "- How can we perform nuclei segmentation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a1aab",
   "metadata": {},
   "source": [
    "- Basic image handling: reading in image files, and how to convert them into numerical arrays.\n",
    "- Measurement of pixel intensity, ploting these data as histograms, and learning how to equalise these intensity values across an image.\n",
    "- OpenCV to normalise pixel intensity values of one channel relative to another, to normalise an image.\n",
    "- Constructing scatter plots to observe correlations between the pixel intensities of each channel.\n",
    "- Clustering multiple data points, helping to clearly identify distinct distributions.\n",
    "- Selecting and zoning in on specific parts of an image, for analyses.\n",
    "- Implementation of image clustering.\n",
    "- Counting nuclei *via* image segmentation, targeting cell nuclei using StarDist.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f0f68f",
   "metadata": {},
   "source": [
    "## **Why is Image Analysis important?**\n",
    "Image Analysis extracts meaningful data, and can be useful in many different applications. Such as:\n",
    "\n",
    "1. Analysing fluorescent digital slides, which can help quantitate the area of staining of a particular biomarker.\n",
    "\n",
    "2. Image analysis tools can help to automate repetitive processes, and provide quantitative data that is accurate and repeatable, telling you about each slide: beyond the capabilities of manual microscopy.\n",
    "\n",
    "3. Image analyses performed on fluorscent micrographs can help answer questions such as: \n",
    "\n",
    "  - How much area of a tissue sample is stained for a particular biomarker? \n",
    "\n",
    "  - What is the average intensity of a biomarker staining throughout the sample? \n",
    "  \n",
    "  - Do multiple biomarkers co-localize in the tissue, or within individual cells? If so, to what extent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed811d7f",
   "metadata": {},
   "source": [
    "### **What is an image?**\n",
    "  \n",
    "Images are essentially a matrix in which each index is assigned a number, representing the value at that specifc point. \n",
    "<p style='text-align: justify;'>\n",
    "These indices, more commonly called pixels, are a measure of how many photons hit that particular position on the camera sensor. As these images are matrices, we can do many of the same mathematical operations on images as we do on matrices. However, we don't want to manually generate every image by populating a matrix. This means that we will have to have some manner of interacting with image files on our computer, programmatically. \n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "For all scientific images, you should save them in a file format that minimizes the data lost due to compression. In the field of image analysis, the most common file format you will run into is the Tagged Image File Format (.tiff or .tif) which is a lossless compression format. \n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "In this tutorial, we will learn how to load, manipulate and extract quantitative data from microscopy images. The data set used in this experiment has been taken from [Broad Bioimage Benchmark Collection](https://bbbc.broadinstitute.org/BBBC008) and presents *Human HT29 colon-cancer cells*.\n",
    "</p>\n",
    "\n",
    "### **Images in dataset**\n",
    "<p style='text-align: justify;'>\n",
    "The image set consists of 12 individual images. The samples were stained with Hoechst (channel 1), pH3 (channel 2), and phalloidin (channel 3). Hoechst labels DNA, which is present in the nucleus. Phalloidin labels actin, which is present in the cytoplasm. The last stain - pH3 - is used to indicate cells in mitosis, and is irrelevant for segmentation and counting, so this channel will be ommitted. In this tutorial we will be exploring channel 1 for nuclei, and channel 3 for the cytoplasm.\n",
    "</p>\n",
    "\n",
    "### **Packages**\n",
    "<p style='text-align: justify;'>\n",
    "There are several image processing libraries available in Python, such as *Python Imaging Library*, *Sci-kit Image*, *Scipy.ndimage* and *Open CV*. While each have their own features, they all have the same (or at least very similar) utilities. In this lesson, we will be using the *Python Imaging Library* and *Sci-kit Image*, as these have everything we require, packaged together in easy-to-use functions.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "The *Python Imaging Library (PIL)* has a large number of modules and submodules. Each submodule has an array of functions assoicated with a certain type of operation. For example, we will be using module **Image** which provides a number of factory functions, including functions to load images from files, and to create new images.\n",
    "</p>\n",
    "For other basic processing and plotting in Python, the scientific packages Numpy and Matplotlib are needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf3e2e",
   "metadata": {},
   "source": [
    "In a blank Jupyter Notebook, open a new cell, and write your own Python code to import the following packages and modules:\n",
    "\n",
    "- Import pyplot as plt from matplotlib\n",
    "- Import numpy\n",
    "- Import glob\n",
    "- From PIL, we need the Image module\n",
    "- From skimage, we need the exposure module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the packages\n",
    "\n",
    "from PIL import Image # Python Image Library (Pillow)\n",
    "from skimage import exposure\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352aa49",
   "metadata": {},
   "source": [
    "## Loading images\n",
    "To begin, let's read in all the images from a directory, and then load an image into memory using the **open** function from the **Image** module of *Python Imaging Library*. This function opens and identifies the given image file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6338e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get image files\n",
    "\n",
    "ch1_files = glob.glob('data/human_ht29_colon_cancer_2_images/*_channel1.tif') # channel 1 image - nuclei\n",
    "ch3_files = glob.glob('data/human_ht29_colon_cancer_2_images/*_channel3.tif') # channel 3 image - cytoplasm\n",
    "\n",
    "## Check they are correctly paired\n",
    "print(ch1_files[0], \":\", ch3_files[0], \"\\n\")\n",
    "\n",
    "## Sort files\n",
    "\n",
    "ch1_files.sort()\n",
    "ch3_files.sort()\n",
    "\n",
    "## Check they are correctly paired\n",
    "print(\"Correctly paired .... \\n\")\n",
    "print(ch1_files[0], \":\", ch3_files[0], \"\\n\") # First file\n",
    "print(ch1_files[11], \":\", ch3_files[11], \"\\n\") # Last file\n",
    "\n",
    "## Load the first image\n",
    "\n",
    "im1 = Image.open(ch1_files[0])\n",
    "im3 = Image.open(ch3_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767b607",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "data/human_ht29_colon_cancer_2_images/AS_09125_050116000001_A24f00d0_slice6_channel1.tif : data/human_ht29_colon_cancer_2_images/AS_09125_050116000001_A24f00d0_slice1_channel3.tif \n",
    "\n",
    "Correctly paired .... \n",
    "\n",
    "data/human_ht29_colon_cancer_2_images/AS_09125_050116000001_A24f00d0_slice1_channel1.tif : data/human_ht29_colon_cancer_2_images/AS_09125_050116000001_A24f00d0_slice1_channel3.tif \n",
    "\n",
    "data/human_ht29_colon_cancer_2_images/AS_09125_050116000001_L15f00d0_slice6_channel1.tif : data/human_ht29_colon_cancer_2_images/AS_09125_050116000001_L15f00d0_slice6_channel3.tif \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show Data\n",
    "\n",
    "fig, axs = plt.subplots(1,2, dpi = 300, figsize=(6, 6))\n",
    "\n",
    "axs[0].imshow(im1);\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(im3);\n",
    "axs[1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb7d3aa",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(-0.5, 511.5, 511.5, -0.5)\n",
    "```\n",
    "\n",
    "![](fig/img1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6fe234",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(im1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54157338",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "<class 'PIL.TiffImagePlugin.TiffImageFile'>\n",
    "```\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "The above command shows the type of image object im1. In order to perform any analyses, we need to have the numerical information of an image, which exists as arrays. The arrays are implemented by a package called Numpy, which is foundational to the entire scientific Python ecosystem. As soon as we perform numerical computations, we use data in the form of Numpy arrays.\n",
    "</p>\n",
    "\n",
    "Below, we convert the image object into an array, as follows:\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306cfab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert image to array\n",
    "\n",
    "data1 = np.asarray(im1)\n",
    "data3 = np.asarray(im3)\n",
    "\n",
    "print('Image has', data1.shape[0], 'by', data1.shape[1], 'pixels') # check the shape of array\n",
    "print('')\n",
    "print('Total number of pixels is', data1.size) # check the size of array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522747e",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "Image has 512 by 512 pixels\n",
    "\n",
    "Total number of pixels is 262144\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192e493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the content of the array\n",
    "\n",
    "print(data1)\n",
    "print('')\n",
    "print(data1[10:20, 10:20])\n",
    "print('')\n",
    "print('Array type:', type(data1))\n",
    "print('Data type: ',  data1.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52c2714",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "[[11 11 11 ...  9 10 10]\n",
    " [12 11 11 ... 10  9 11]\n",
    " [11 11 11 ... 10 11 10]\n",
    " ...\n",
    " [11 11 11 ... 10 10 10]\n",
    " [12 11 11 ...  9 10 10]\n",
    " [11 11 11 ... 10 10 10]]\n",
    "\n",
    "[[128 150 137 100  53  27  18  14  12  10]\n",
    " [155 177 172 136  73  38  21  15  11  11]\n",
    " [140 146 154 144  96  50  24  15  13  11]\n",
    " [133 144 139 122  95  55  26  15  13  11]\n",
    " [114 117 122 103  78  46  22  14  11  10]\n",
    " [ 79  87  97  78  54  32  18  12  11  12]\n",
    " [ 48  54  53  46  27  20  14  12  10  11]\n",
    " [ 20  22  23  20  15  13  13  11  11  11]\n",
    " [ 17  16  15  14  13  11  11  11  10  12]\n",
    " [ 12  14  14  13  14  12  12  12  11  11]]\n",
    "\n",
    "Array type: <class 'numpy.ndarray'>\n",
    "Data type:  uint8\n",
    "```\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "The type function has returned numpy.ndarray where nd stands for n-dimensional, as Numpy can handle data of any dimension. The above output only shows us a fraction of the image, with the ```...``` indicating non-displayed values. We can also observe `dtype`, which tells us the type of the pixels within the array. The variable data1 is an array, but its content can vary: we could have floating point values, integers *etc*. Here, uint8 tells us we have unsigned (no negative values) integers in 8 bit, *i.e.* up to $2^8$ different possible pixel values.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "In microscopy, the flourecent images are labelled with three colours: red, green and blue (RGB). An RGB image is simply a stack of three two-dimensional arrays. The image in the first dimension will have a red colour map, the second a green colour map, and the third a blue colour map. In this dataset, channel 1 (nuclei) is labelled red, and channel 3 (cytoplasm) is labelled blue; channel 2 is empty. We will now combine the data of these two images into a single array, and into a single image.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2233174",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Combine data of two images\n",
    "data = np.zeros((data1.shape[0], data1.shape[0], 3)) # 3 two-dimensional arrays; 1 for each channel\n",
    "data[:,:,0] = data1 # Assigning image 1 as first channel\n",
    "data[:,:,2] = data3 # Assigning image 3 as 3rd/last channel\n",
    "\n",
    "# Converting numeric data to image\n",
    "im = Image.fromarray(np.uint8(data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61918a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show combined data\n",
    "fig, axs = plt.subplots(1, 3, dpi = 400)\n",
    "\n",
    "axs[0].imshow(data1);\n",
    "axs[0].axis('off');\n",
    "\n",
    "axs[1].imshow(data3);\n",
    "axs[1].axis('off');\n",
    "\n",
    "axs[2].imshow(im); # Combined image\n",
    "axs[2].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63073151",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(-0.5, 511.5, 511.5, -0.5)\n",
    "```\n",
    "\n",
    "![](fig/img3.png)\n",
    "\n",
    "and enlarged:\n",
    "  \n",
    "```\n",
    "im\n",
    "```\n",
    "![](fig/combined.png)\n",
    "The combined image shows red (nuclei) and blue (cytoplasm) colors.\n",
    "\n",
    "## Data Exploration\n",
    "<p style='text-align: justify;'>\n",
    "In image analysis, one of the most common tasks is to look at the distribution of pixel intensities, and display this as a histogram. For example: we can count how many times each pixel value (or range) appears in an image, and display this as a bar. This allows us to get a quick estimate of the intensities present in an image and to check for problems like saturation. We are going to plot the intensities of channel 1 and 3 as histograms.  \n",
    "</p>\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "We have already demonstrated that we can use Matplotlibâ€™s `imshow` function to display an image. Now we look at a second plotting function from that library, which generates a histogram plt.hist(). We are aiming to compute the histogram on all pixels, and for this we cannot use our 2D image as an input. The image needs to first be flattened into a single series of numbers. This can be done with the ravel() method of the array:\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "fig, axs = plt.subplots(2, sharex=True, sharey=True)\n",
    "\n",
    "axs[0].hist(data1.ravel(), bins=100);\n",
    "axs[0].set_title(\"nuclei\");\n",
    "axs[0].set_xlim(0,40)\n",
    "\n",
    "axs[1].hist(data3.ravel(), bins=100);\n",
    "axs[1].set_title(\"cytoplasm\");\n",
    "axs[1].set_xlim(0,40)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(data1.max(), \":\", data3.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab38ab4",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(0.0, 40.0)\n",
    "(0.0, 40.0)\n",
    "```\n",
    "\n",
    "<img src=\"fig/01-section1-rendered-unnamed-chunk-9-1.png\" width=\"672\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "```{.output}\n",
    "255 : 122\n",
    "```\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "The histograms show that there is a very high intensity of fluorescence for nuclei, relative to the cytoplasm, and this is the reason that red is more evident on the image, compared to blue. The intensities can be equalised by using a function *exposure.equalize_hist* from the **exposure** module in skimage library. This function returns an image which can then be used to plot a histogram.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44415b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equalize histogram \n",
    "from skimage import exposure\n",
    "\n",
    "img_eq = exposure.equalize_hist(data)\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.imshow(img_eq)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48228a7a",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "/home/runner/.virtualenvs/carp-env/lib/python3.10/site-packages/skimage/_shared/utils.py:394: UserWarning: This might be a color image. The histogram will be computed on the flattened image. You can instead apply this function to each color channel, or set channel_axis.\n",
    "  return func(*args, **kwargs)\n",
    "(-0.5, 511.5, 511.5, -0.5)\n",
    "```\n",
    "\n",
    "<img src=\"fig/01-section1-rendered-unnamed-chunk-10-3.png\" width=\"672\" style=\"display: block; margin: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c8a82",
   "metadata": {},
   "source": [
    "## Do it Yourself - Exercise 1\n",
    "\n",
    "Plot a histogram for equalised image, and show intensities of nuclei and cytoplasm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b9e12",
   "metadata": {},
   "source": [
    "## DIY ANSWER\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb14d4c",
   "metadata": {},
   "source": [
    "### **OpenCV**\n",
    "<p style='text-align: justify;'>\n",
    "To demonstrate the vast functionality available in Python, we will take a look at __OpenCV__. OpenCV (Open Source Computer Vision Library: http://opencv.org); an open-source library that includes several hundred computer vision algorithms.\n",
    "</p>\n",
    "<p style='text-align: justify;'>\n",
    "For example, we can use functions to normalise our image data. Mathematical normalisation can be performed on images individually, or with respect to another channel. In the following example, we will normalise channel 3, with respect to channel 1. To do so, we calculate the mean and standard deviation of the pixel intensity of channel 1, and remove them from channel 3.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86d381",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Normalise to a different channel\n",
    "\n",
    "import cv2 # GB - Explanation of package cv2\n",
    "#print(cv2._version_)\n",
    "\n",
    "# Calculate mean and STD\n",
    "mean, STD = cv2.meanStdDev(data1)\n",
    "\n",
    "# Clip frame to lower and upper STD\n",
    "offset = 1\n",
    "offset_nuclei = np.clip(im, mean - STD, mean + STD).astype(np.uint8)\n",
    "\n",
    "# Normalise to range\n",
    "result = cv2.normalize(offset_nuclei, np.uint8(data), 0, 255, norm_type=cv2.NORM_MINMAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be405dd0",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d52f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalised image\n",
    "plt.figure(dpi=200)\n",
    "plt.imshow(result);\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a493cb",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(-0.5, 511.5, 511.5, -0.5)\n",
    "```\n",
    "\n",
    "<img src=\"fig/01-section1-rendered-unnamed-chunk-12-5.png\" width=\"672\" style=\"display: block; margin: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af84a9",
   "metadata": {},
   "source": [
    "## Do it Yourself - Exercise 2\n",
    "\n",
    "Normalise the combined image with respect to cytoplasm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c7f29",
   "metadata": {},
   "source": [
    "## DIY ANSWER\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff6aae",
   "metadata": {},
   "source": [
    "## Tea Break\n",
    "\n",
    "## Data Visualisation\n",
    "\n",
    "Now we can plot histograms of the normalised intensities in channel 1, channel 3, and the background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms\n",
    "fig, axs = plt.subplots(3, sharex=True, sharey=True)\n",
    "\n",
    "axs[0].hist(result[:,:,0].ravel(), bins=30);\n",
    "axs[0].set_title('Nuclei')\n",
    "\n",
    "axs[1].hist(result[:,:,2].ravel(), bins=30);\n",
    "axs[1].set_title('Cytoplasm')\n",
    "\n",
    "axs[2].hist(result[:,:,2].ravel() - result[:,:,0].ravel(), bins=30);\n",
    "axs[2].set_title('Cytoplasm - Nuclei')\n",
    "\n",
    "fig.tight_layout();\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b307499",
   "metadata": {},
   "source": [
    "<img src=\"fig/01-section1-rendered-unnamed-chunk-13-7.png\" width=\"672\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "The third histogram shows everything (background signal), except for nuclei and cytoplasm.\n",
    "\n",
    "To investigate correlations between the intensities, we can look at a scatter plot (below left, once you run the cell, below). This type of plot is generally useful for exploratory data analysis. \n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "We can see that the scatter plot is quite dense, and it is not really possible to see how many points are present, at a given location. We can display this information with the two-dimensional histogram (below centre). This clearly shows the count of points that fall into a certain square of intensities. (Note the thin, yellow area towards the right margin).\n",
    "</p>\n",
    "\n",
    "### **Seaborn**\n",
    "<p style='text-align: justify;'>\n",
    "From a python plotting library called [seaborn](https://seaborn.pydata.org), we can import a function called `kdeplot`, which allows us to make a contour plot of the intensities. The contour plot confirms that there are at least two distinct distributions. Such a finding might inspire grouping the intensities into different categories, or clusters.\n",
    "</p>\n",
    "\n",
    "## Note\n",
    "This plot might take a bit longer to run, as there are a large number of data points. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#```{python}\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Scatter plot\n",
    "ax[0].scatter(result[:,:,0].flatten(), result[:,:,2].flatten());\n",
    "\n",
    "# # 2D Histogram\n",
    "ax[1].hist2d(result[:,:,0].flatten(), result[:,:,2].flatten(), bins=50, vmax=50);\n",
    "\n",
    "from seaborn import kdeplot\n",
    "\n",
    " # Density Plot\n",
    "kdeplot(x=result[:,:,0].flatten(), y=result[:,:,2].flatten(), ax=ax[2]);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eaf9f4",
   "metadata": {},
   "source": [
    "## Selecting part of an image\n",
    "\n",
    "It is also possible to analyse a small region of an image. The image can be cropped, and then analyses can be performed on the selected region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotating the regions\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "x, y, w, h = 450, 10, 60, 50\n",
    "\n",
    "fig, ax = plt.subplots(dpi = 100)\n",
    "\n",
    "plt.imshow(result)\n",
    "\n",
    "ax.add_patch(Rectangle((x,y), w, h, edgecolor=\"w\", fill=False));\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813f7a4",
   "metadata": {},
   "source": [
    "<img src=\"fig/01-section1-rendered-unnamed-chunk-15-9.png\" width=\"672\" style=\"display: block; margin: auto;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f133d65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot what's in box\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "plt.imshow(result[y:y+h, x:x+w,]);\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a673731",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(-0.5, 59.5, 49.5, -0.5)\n",
    "```\n",
    "\n",
    "<img src=\"fig/01-section1-rendered-unnamed-chunk-16-11.png\" width=\"672\" style=\"display: block; margin: auto;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adcde54",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[y:y+h, x:x+w, 0].shape\n",
    "result[:,:,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc4335",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(50, 60)\n",
    "(512, 512)\n",
    "```\n",
    "\n",
    "The region in the rectangle does not have any cells. Plotting a histogram for this region versus full image will result in the following plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5df51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram \n",
    "fig, axs = plt.subplots(2, sharex=True)\n",
    "\n",
    "axs[0].hist(result[:,:,0].ravel(), bins=30, label='intensity in all image');\n",
    "axs0_2 = axs[0].twinx()\n",
    "axs0_2.hist(result[y:y+h,x:x+w,0].ravel(), bins=3, color='r', alpha=0.3);\n",
    "axs[0].set_title('Nuclei')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].hist(result[:,:,2].ravel(), bins=30);\n",
    "axs1_2 = axs[1].twinx()\n",
    "axs1_2.hist(result[y:y+h,x:x+w,2].ravel(), bins=3, color='r', alpha=0.3, label='intensity in ROI');\n",
    "axs[1].set_title('Cytoplasm')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af4722",
   "metadata": {},
   "source": [
    "<img src=\"fig/01-section1-rendered-unnamed-chunk-18-13.png\" width=\"672\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "This plot compares the intensity of the pixels in the total image (blue) to those in the region of interest (ROI) bounded by the rectangle. If we move the ROI over cells, we can see how this intensity changes.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026edfc1",
   "metadata": {},
   "source": [
    "## Do it Yourself - Exercise 3\n",
    "\n",
    "Move the rectangle towards left so that it have a few cells. Now plot the histogram again and see the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba1ca6",
   "metadata": {},
   "source": [
    "## DIY ANSWER\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523eb573",
   "metadata": {},
   "source": [
    "## Do it Yourself - Exercise 4\n",
    "\n",
    "Repeat this process for other images (in the folder) and save 3-4 examples of cells with nuclei and cytoplasm marked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274c814d",
   "metadata": {},
   "source": [
    "## DIY ANSWER\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13e6f2",
   "metadata": {},
   "source": [
    "## Lunch Break\n",
    "\n",
    "## Image clustering\n",
    "\n",
    "[Clustering Methods](https://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29891fdb",
   "metadata": {},
   "source": [
    "![Methods Comparison](fig/ClusterMethods.png)\n",
    "![Success Comparison](fig/Clustering_comparison.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data for clustering\n",
    "\n",
    "img1 = result[:,:,0].reshape(-1, 1)\n",
    "img2 = result[:,:,2].reshape(-1, 1)\n",
    "\n",
    "all_imgs = np.concatenate([img1, img2], axis=1)\n",
    "\n",
    "all_imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89edbb39",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(262144, 2)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada6f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the clustering class\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Cluster the images and obtain the labels for each pixel\n",
    "n_components = 3\n",
    "\n",
    "RANDOM_STATE = 12345\n",
    "\n",
    "gmm = GaussianMixture(n_components=n_components, \n",
    "                      random_state=RANDOM_STATE)\n",
    "\n",
    "all_img_labels = gmm.fit_predict(all_imgs)\n",
    "\n",
    "all_img_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2fdf07",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d238808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.scatter(img1[img1 > 0], img2[img2 > 0], c=all_img_labels, s=50)\n",
    "\n",
    "ax.set_xlabel('Image 1', fontsize=16)\n",
    "ax.set_ylabel('Image 2', fontsize=16);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58c27d",
   "metadata": {},
   "source": [
    "<img src=\"fig/01-section1-rendered-unnamed-chunk-21-15.png\" width=\"576\" style=\"display: block; margin: auto;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c5984",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "\n",
    "mask = (data1>0) & (data3>0) \n",
    "\n",
    "all_img_labels_mapped = zeros(data1.shape)\n",
    "\n",
    "all_img_labels_mapped[mask] = all_img_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.imshow(all_img_labels_mapped);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b75a784",
   "metadata": {},
   "source": [
    "<img src=\"fig/01-section1-rendered-unnamed-chunk-23-17.png\" width=\"576\" style=\"display: block; margin: auto;\" />\n",
    "\n",
    "The result depends critically on the specified number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392952e4",
   "metadata": {},
   "source": [
    "## Do it Yourself - Exercise 5\n",
    "\n",
    "Change n_components to 2, 4, 5 and 6 and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c29676",
   "metadata": {},
   "source": [
    "## DIY ANSWER\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b062c04",
   "metadata": {},
   "source": [
    "## Do it Yourself - Exercise 6\n",
    "\n",
    "Apply the Kmeans algorithm to the images and compare the result of the clustering for different choices of n_components.\n",
    "\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)# To import:\n",
    "\n",
    "```\n",
    "# To import:\n",
    "from sklearn.cluster import KMeans\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a62a0",
   "metadata": {},
   "source": [
    "## DIY ANSWER\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393cc4b7",
   "metadata": {},
   "source": [
    "## Tea Break\n",
    "\n",
    "## Segmentation\n",
    "\n",
    "Image segmentation allows us to locate objects and boundaries (lines, curves, *etc.*) within an image. Conventional methods make use of a threshold value to distinguish between different regions and/or cells. In this lesson, we will use a Python package called [**StarDist**](https://github.com/stardist/stardist), which makes use of deep learning methods to perform nuclei segmentation. It is designed particularly for nuclei segmentation. **StarDist** comes with pretrained models, that are likely suitable for your micrograph images. However, it is also possible to train your model specifically for your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa1f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load as images - Using Raw images\n",
    "\n",
    "data1 = Image.open(ch1_files[0])\n",
    "\n",
    "nuclei = np.zeros(np.shape(im))\n",
    "nuclei[:, :, 0] = data1\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.imshow(nuclei.astype(int))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44c76ab",
   "metadata": {},
   "source": [
    "```{.output}\n",
    "(-0.5, 511.5, 511.5, -0.5)\n",
    "```\n",
    "\n",
    "<img src=\"fig/01-section1-rendered-unnamed-chunk-24-19.png\" width=\"672\" style=\"display: block; margin: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c494d96f",
   "metadata": {},
   "source": [
    "```\n",
    "from stardist.models import StarDist2D\n",
    "from stardist import random_label_cmap\n",
    "\n",
    "from csbdeep.utils import Path, normalize\n",
    "\n",
    "axis_norm = (0,1)   # normalize channels independently\n",
    "img = normalize(nuclei[:,:,0], 1,99.8, axis=axis_norm)\n",
    "\n",
    "model = StarDist2D.from_pretrained('2D_versatile_fluo')\n",
    "lbl_cmap = random_label_cmap()\n",
    "```\n",
    "<p style='text-align: justify;'>\n",
    "By running this Python code, you will see this message. This suggests that everything is working fine. The package **StarDist** requires the installation of a deep learning package **tensorflow**.\n",
    "</p>\n",
    "\n",
    "**Output**\n",
    "```\n",
    "Found model '2D_versatile_fluo' for 'StarDist2D'.\n",
    "Loading network weights from 'weights_best.h5'.\n",
    "Loading thresholds from 'thresholds.json'.\n",
    "Using default values: prob_thresh=0.479071, nms_thresh=0.3.\n",
    "```\n",
    "\n",
    "<p style='text-align: justify;'>\n",
    "In order to access pretrained models, the function *StarDist2D.from_pretrained()* provides a list from which a suitable model can be selected. There are two main types of models: one for fluorescent images, and the other for brightfield images. For our image, **StarDist** will perform nuclei prediction using *2D_versatile_fluo*.\n",
    "</p>\n",
    "\n",
    "The availbilty of pretrained models can be checked as follows:\n",
    "\n",
    "```\n",
    "StarDist2D.from_pretrained()\n",
    "```\n",
    "\n",
    "**Output**\n",
    "```\n",
    "There are 4 registered models for 'StarDist2D':\n",
    "\n",
    "Name                  Alias(es)\n",
    "             \n",
    "'2D_versatile_fluo'   'Versatile (fluorescent nuclei)'\n",
    "'2D_versatile_he'     'Versatile (H&E nuclei)'\n",
    "'2D_paper_dsb2018'    'DSB 2018 (from StarDist 2D paper)'\n",
    "'2D_demo'             None\n",
    "```\n",
    "\n",
    "```\n",
    "# Using pre-trained network to predict nuclei\n",
    "labels, details = model.predict_instances(img)\n",
    "```\n",
    "\n",
    "```\n",
    "## Show data\n",
    "\n",
    "fig, axs  = plt.subplots(1,2, dpi=200)\n",
    "axs[0].imshow(nuclei.astype(int))\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(nuclei.astype(int))\n",
    "axs[1].imshow(labels, cmap=lbl_cmap,alpha=0.5)\n",
    "axs[1].axis('off')\n",
    "\n",
    "```\n",
    "![](fig/seg1.png)\n",
    "<p style='text-align: justify;'>\n",
    "The image on the right is a segmented image, with correctly identified and predicted nuclei. This image is also referred to as a masked image. The next step will allow us to count the number of nuclei in this image, which is performed as follows:\n",
    "</p>\n",
    "\n",
    "```\n",
    "# Enumerate nuclei\n",
    "plt.figure(dpi=100)\n",
    "plt.imshow(nuclei.astype(int))\n",
    "\n",
    "for i, xy in enumerate(details['points']):\n",
    "    plt.annotate(str(i+1), [xy[1]+5,xy[0]], color='w', size=5)\n",
    "plt.axis('off')\n",
    "\n",
    "print(\"The total number of nuclei in the image are: \", len(details['points']))\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "The total number of nuclei in the image are:  168\n",
    "```\n",
    "\n",
    "![](fig/seg3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17c741",
   "metadata": {},
   "source": [
    "## Do it Yourself - Exercise 7\n",
    "\n",
    "Repeat segmentation for other images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4c899",
   "metadata": {},
   "source": [
    "## DIY ANSWER\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72de1d8",
   "metadata": {},
   "source": [
    "## Do it Yourself - Exercise 8\n",
    "\n",
    "Repeat segmentation for all 12 images and plot number of nuclei per image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32f4876",
   "metadata": {},
   "source": [
    "## DIY ANSWER\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d006aae",
   "metadata": {},
   "source": [
    "## Do it Yourself - Exercise 9\n",
    "\n",
    "Repeat segmentation for cytoplasm channel, what do you notice? Explain your observation for any pecularities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec7e8b",
   "metadata": {},
   "source": [
    "## DIY ANSWER\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125f445",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'>\n",
    "If you have done the above exercise of using **stardist** to perform segmentation for the cytoplasm channel, then you may have noticed pecularities. These can be justified by StarDist really only being designed for accurate segmentation of nuclei. However, if you are interested in cytoplasm segmentation, then using **cellpose** is better suited for this application. This Python package is more specialised, and better apllied to cell and nucleus segmentation. After completing this workshop, you will observe that **stardist** and **cellpose** both work in a similar fashion. For more information and documentation, please follow this [link](https://github.com/MouseLand/cellpose).\n",
    "</p>\n",
    "\n",
    "## Summary and Conclusions\n",
    "<p style='text-align: justify;'>\n",
    "In undertaking this workshop, we have learned to import, explore and analyse fluorescence microscopy images, and how to perform some basic image analyses using several packages, in Python. We have also learned how to perform clustering analyses, through implementing unsupervised machine learning algorithms (GMM). Further to this, we provided an introduction into the use of deep learning algorithms for use in nuclei segmentation, which can provide a fast, accurate and powerful count of the nuclei present in an image; this can be invaluable for analysing large amounts of data. Following the guided links, you can start to explore more advanced image analysis techniques, and *Learn To Discover* how these can help you further your own work, and reserach.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ae271",
   "metadata": {},
   "source": [
    "- Use `.md` files for episodes when you want static content\n",
    "- Use `.Rmd` files for episodes when you need to generate output\n",
    "- Run `sandpaper::check_lesson()` to identify any issues with your lesson\n",
    "- Run `sandpaper::build_lesson()` to preview your lesson locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627fca5",
   "metadata": {},
   "source": [
    "[r-markdown]: https://rmarkdown.rstudio.com/"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
